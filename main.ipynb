{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbNCG5mfg4pM"
      },
      "source": [
        "**Importing the necessary libraries for data manipulation, preprocessing, model building, and evaluation. Which also includes pandas for data handling, NumPy for numerical operations, and modules from scikit-learn for machine learning tasks. We also import SMOTE from imbalanced-learn to handle class imbalance and additional libraries for model performance metrics and sparse matrix handling.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "paJ2dQiZPVlc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File size: 470.67 MB\n"
          ]
        }
      ],
      "source": [
        "file_path = './content/Fraud.csv'\n",
        "file_size = os.path.getsize(file_path)\n",
        "file_size_mb = file_size / (1024 * 1024)\n",
        "\n",
        "print(f\"File size: {file_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PFH8RGneHPA"
      },
      "source": [
        "**Loading the dataset into a pandas DataFrame from a CSV file. This is the initial step in data preprocessing where we read the data into a format suitable for analysis and manipulation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E8714xUdPlDc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./content/Fraud.csv', nrows=1000000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0PSjWhDeU6r"
      },
      "source": [
        "**Here we define the columns which are numerical and categorical. This distinction is crucial because the preprocessing steps for numerical and categorical data differ significantly.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ax7BqG6aP49I"
      },
      "outputs": [],
      "source": [
        "numerical_columns = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
        "categorical_columns = ['type', 'nameOrig', 'nameDest']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTWfY3COegYT"
      },
      "source": [
        "**we initialize imputer objects for handling missing values in the dataset. For numerical columns, we use the mean value of the column to fill missing values. For categorical columns, we use the most frequent value.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YlG6ygVsP8Df"
      },
      "outputs": [],
      "source": [
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o4dUmotep31"
      },
      "source": [
        "**Then we apply the imputers to the numerical and categorical columns to fill all the missing values. This step ensures that our dataset does not have any missing values which could cause errors in subsequent steps.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ozOBGW1vP-kc"
      },
      "outputs": [],
      "source": [
        "df[numerical_columns] = num_imputer.fit_transform(df[numerical_columns])\n",
        "df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNT9FsrPeze_"
      },
      "source": [
        "**Then by initializing a OneHotEncoder to convert categorical variables into a format that can be used by machine learning models. One-hot encoding transforms categorical values into a series of binary columns.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVyi1CivQAcX",
        "outputId": "7c9e9c24-c710-42a3-9157-5bdefa65b441"
      },
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder(sparse_output=True)\n",
        "encoded_cats = encoder.fit_transform(df[categorical_columns])\n",
        "encoded_cat_columns = encoder.get_feature_names_out(categorical_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCbcMRGpe-gp"
      },
      "source": [
        "**By converting the encoded categorical data into a DataFrame and concatenate it with the original DataFrame after dropping the original categorical columns. This step integrates the encoded categorical data with our dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lSTkv5jnQCdx"
      },
      "outputs": [],
      "source": [
        "df_encoded = pd.DataFrame.sparse.from_spmatrix(encoded_cats, columns=encoded_cat_columns)\n",
        "df = df.drop(categorical_columns, axis=1)\n",
        "df = pd.concat([df, df_encoded], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn2ffy5hfJiV"
      },
      "source": [
        "**By separating the features and the target variable from the dataset. Then store the features in X and the target variable (fraud label) in y.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k2TXalx6QEj2"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['isFraud', 'isFlaggedFraud'], axis=1)\n",
        "y = df['isFraud']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZhlzdxFgusT"
      },
      "source": [
        "**For handling the missing values in the target variable by filling them with zeros and converting the target variable to an integer type. This ensures that our target variable is in the correct format for model training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YEq3j6wzQITP"
      },
      "outputs": [],
      "source": [
        "y = y.fillna(0).astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbSQjBHbg6FL"
      },
      "source": [
        "**The indices of fraudulent and non-fraudulent transactions are noted. This allows us to create a balanced subset of the data for initial training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Yq7WcnhQKUM"
      },
      "outputs": [],
      "source": [
        "fraud_indices = y[y == 1].index\n",
        "non_fraud_indices = y[y == 0].sample(n=len(fraud_indices), random_state=42).index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40EF9vyYkbEM"
      },
      "source": [
        "**By combining the indices of fraudulent and non-fraudulent transactions to create a balanced subset of our data. This helps in training the model effectively without being biased towards non-fraudulent transactions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HzG3V3wSRCdw"
      },
      "outputs": [],
      "source": [
        "sample_indices = fraud_indices.union(non_fraud_indices)\n",
        "X_sample = X.loc[sample_indices]\n",
        "y_sample = y.loc[sample_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GXePnOjkiQS"
      },
      "source": [
        "**By applying the SMOTE technique to our balanced subset to further address class imbalance. SMOTE generates synthetic examples of the minority class (fraudulent transactions) to create a more balanced dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUMt74D1SMep",
        "outputId": "b8f5236b-b175-47fb-df61-c21920fa9ccb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rithik/Documents/raju/env/lib/python3.11/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_sample, y_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaVnD18kkmam"
      },
      "source": [
        "**Standardizing the features by removing the mean and scaling to unit variance. This is necessary for many machine learning algorithms to perform optimally.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7_S3I2zSQFI",
        "outputId": "c7853641-9a26-4556-df79-ce973a565ba5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rithik/Documents/raju/env/lib/python3.11/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n",
            "/Users/rithik/Documents/raju/env/lib/python3.11/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler(with_mean=False)\n",
        "X_scaled = scaler.fit_transform(X_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjVCYqKbkwGZ"
      },
      "source": [
        "**I planned to use RandomForest Classifier for this fraudulent transactions model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G8Nj4Q-KSbd1"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c11C1Chqk978"
      },
      "source": [
        "**By training and evaluating each model using GridSearchCV to find the best hyperparameters. I splited the data into training and testing sets, perform hyperparameter tuning, and evaluate the models using classification metrics and AUC-ROC scores.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrGHaTTySgXq",
        "outputId": "3faf8032-d26d-480e-9362-91584abb6ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RandomForest model...\n",
            "Best parameters for RandomForest: {'max_depth': 30, 'n_estimators': 100}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.85      0.90       156\n",
            "           1       0.87      0.97      0.92       165\n",
            "\n",
            "    accuracy                           0.91       321\n",
            "   macro avg       0.92      0.91      0.91       321\n",
            "weighted avg       0.92      0.91      0.91       321\n",
            "\n",
            "AUC-ROC for RandomForest: 0.9111305361305362\n"
          ]
        }
      ],
      "source": [
        "print(\"Training RandomForest model...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Hyperparameter Tuning\n",
        "param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30]}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(f\"Best parameters for RandomForest: {grid_search.best_params_}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "auc_roc = roc_auc_score(y_test, y_pred)\n",
        "print(f\"AUC-ROC for RandomForest: {auc_roc}\")\n",
        "\n",
        "result = {\n",
        "    'best_params': grid_search.best_params_,\n",
        "    'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
        "    'auc_roc': auc_roc\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
